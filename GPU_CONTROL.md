# üéÆ Controle de GPU em Runtime

Este projeto permite **ligar e desligar a GPU** dinamicamente sem reiniciar o servidor.

## üöÄ Como Funcionar

### 1. Via Interface Web

**Bot√£o de Status GPU** (ao lado do seletor de modelo):

- **üñ•Ô∏è CPU** - Indica que est√° rodando em CPU
- **üöÄ GPU** - Indica que est√° rodando em GPU

**Para alternar:**
- Clique no bot√£o de status
- O sistema muda automaticamente entre CPU e GPU
- Uma mensagem de confirma√ß√£o aparece no chat

**Status em Tempo Real:**
- Hover sobre o bot√£o mostra informa√ß√µes da GPU
- Atualiza√ß√£o autom√°tica a cada 10 segundos

### 2. Via Script Python

```bash
# Ver status atual
python scripts/gpu_control.py status

# Mudar para GPU
python scripts/gpu_control.py cuda

# Mudar para CPU
python scripts/gpu_control.py cpu

# Alternar automaticamente
python scripts/gpu_control.py toggle
```

**Exemplo de sa√≠da:**
```
============================================================
üîç Status GPU/CUDA
============================================================
Device Atual: CUDA
CUDA Dispon√≠vel: ‚úÖ Sim

GPU: NVIDIA GeForce RTX 3080
Mem√≥ria Total: 10.00 GB
Mem√≥ria Alocada: 0.45 GB
Mem√≥ria Reservada: 0.50 GB
CUDA Version: 12.4
============================================================
```

### 3. Via API REST

**GET /gpu/status** - Obter status atual
```bash
curl http://localhost:8000/gpu/status
```

Resposta:
```json
{
  "device": "cuda",
  "cuda_available": true,
  "gpu_name": "NVIDIA GeForce RTX 3080",
  "gpu_memory_total": 10.0,
  "gpu_memory_allocated": 0.45,
  "gpu_memory_reserved": 0.50,
  "cuda_version": "12.4"
}
```

**POST /gpu/set?device=cuda** - Mudar para GPU
```bash
curl -X POST "http://localhost:8000/gpu/set?device=cuda"
```

**POST /gpu/set?device=cpu** - Mudar para CPU
```bash
curl -X POST "http://localhost:8000/gpu/set?device=cpu"
```

**POST /gpu/toggle** - Alternar automaticamente
```bash
curl -X POST http://localhost:8000/gpu/toggle
```

Resposta:
```json
{
  "status": "success",
  "message": "Device alterado de cpu para cuda",
  "old_device": "cpu",
  "new_device": "cuda"
}
```

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente (.env)

```env
# Controle de GPU
USE_GPU=1              # 1 = habilitado, 0 = desabilitado
TORCH_DEVICE=auto      # auto, cpu ou cuda

# FAISS GPU (opcional)
USE_FAISS_GPU=1        # 1 = usa GPU para √≠ndice FAISS
```

### Requisitos

Para usar GPU, voc√™ precisa:

1. ‚úÖ **GPU NVIDIA** compat√≠vel com CUDA
2. ‚úÖ **CUDA Toolkit** instalado (11.8+ ou 12.x)
3. ‚úÖ **PyTorch com CUDA** instalado:
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
   ```

Verificar instala√ß√£o:
```bash
python scripts/check_gpu.py
```

## üìä Quando Usar Cada Modo

### üñ•Ô∏è CPU Mode
**Use quando:**
- Quer economizar energia
- GPU est√° sendo usada por outro processo
- Testando compatibilidade
- Rodando em m√°quina sem GPU

**Performance:**
- Embeddings: ~100 versos/segundo
- Busca FAISS: ~50ms por query

### üöÄ GPU Mode
**Use quando:**
- Quer m√°xima performance
- Processando grandes volumes de texto
- GPU dispon√≠vel e ociosa
- Produ√ß√£o com alta demanda

**Performance:**
- Embeddings: ~500-1000 versos/segundo (5-10x mais r√°pido)
- Busca FAISS: ~5-10ms por query (5-10x mais r√°pido)

## üîÑ Comportamento

### O que acontece ao alternar:

1. **Modelo de Embeddings** √© recarregado no novo device
2. **√çndice FAISS** permanece em mem√≥ria (n√£o √© recarregado)
3. **Conex√£o permanece ativa** (sem rein√≠cio do servidor)
4. **Cache √© mantido** (n√£o √© limpo)

### Mem√≥ria

**CPU ‚Üí GPU:**
- Consome mem√≥ria GPU adicional
- Se falhar por falta de mem√≥ria, reverte para CPU

**GPU ‚Üí CPU:**
- Libera mem√≥ria GPU automaticamente
- Sempre funciona

## üêõ Troubleshooting

### "CUDA not available"

GPU n√£o est√° configurada. Execute:
```bash
python scripts/check_gpu.py
```

E siga as instru√ß√µes.

### "CUDA out of memory"

**Solu√ß√£o 1:** Limpar cache GPU
```python
import torch
torch.cuda.empty_cache()
```

**Solu√ß√£o 2:** Fechar outros programas usando GPU

**Solu√ß√£o 3:** Usar CPU mode:
```bash
python scripts/gpu_control.py cpu
```

### Altern√¢ncia n√£o funciona

Verifique logs do servidor:
```bash
# Terminal onde o servidor est√° rodando mostrar√°:
# ‚úì Modelo carregado na GPU: NVIDIA GeForce RTX 3080
# ou
# ‚úì Modelo carregado em CPU
```

## üìù Logs

O servidor mostra informa√ß√µes detalhadas ao alternar:

```
Carregando modelo paraphrase-multilingual-mpnet-base-v2...
‚úì Modelo carregado na GPU: NVIDIA GeForce RTX 3080
  ‚Ä¢ Mem√≥ria GPU: 10.00 GB
  ‚Ä¢ CUDA Version: 12.4
```

## üéØ Dicas

1. **Mantenha GPU habilitada** em produ√ß√£o para melhor performance
2. **Use CPU** durante desenvolvimento se GPU estiver ocupada
3. **Monitore uso de mem√≥ria** com `nvidia-smi -l 1`
4. **Alterne via web UI** para conveni√™ncia
5. **Use script** para automa√ß√£o/testes

## üìö Links Relacionados

- [GPU_SETUP.md](./GPU_SETUP.md) - Guia completo de instala√ß√£o
- [scripts/check_gpu.py](./scripts/check_gpu.py) - Verifica√ß√£o de GPU
- [scripts/gpu_control.py](./scripts/gpu_control.py) - Controle via terminal

---

**‚ú® Agora voc√™ tem controle total sobre CPU/GPU em tempo real!**
