# AN Agent - Bible Study One Web

Agente de IA para estudos bÃ­blicos profundos com **detecÃ§Ã£o de intertextualidade** usando NLP, embeddings semÃ¢nticos e mÃºltiplos modelos de linguagem (OpenAI, Anthropic, Cohere, Hugging Face).

## âœ¨ Funcionalidades

- ğŸ” **Busca SemÃ¢ntica**: Encontre versos similares usando embeddings avanÃ§ados
- ğŸ”— **DetecÃ§Ã£o de Intertextualidade**: Descubra conexÃµes entre passagens bÃ­blicas
- ğŸ¤– **MÃºltiplos LLMs**: Suporte para OpenAI, Anthropic (Claude), Cohere e Hugging Face
- ğŸ“š **Corpus Original**: AnÃ¡lise em grego (SBLGNT) e hebraico (planejado)
- âš¡ **FAISS**: Busca ultra-rÃ¡pida com indexaÃ§Ã£o vetorial
- ğŸŒ **API REST**: Endpoints FastAPI prontos para integraÃ§Ã£o

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                          # FastAPI application
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ bible_service.py            # ServiÃ§o principal com LLMs
â”‚   â”‚   â”œâ”€â”€ corpus_processor.py         # Processamento de textos bÃ­blicos
â”‚   â”‚   â””â”€â”€ intertextuality_engine.py   # Motor de busca semÃ¢ntica
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ index.html                  # Interface web
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup_corpus.py                 # Script de setup inicial
â”œâ”€â”€ data/                               # Corpus processado (gerado)
â”œâ”€â”€ models/                             # Modelos baixados (gerado)
â”œâ”€â”€ indexes/                            # Ãndices FAISS (gerado)
â”œâ”€â”€ DocumentaÃ§Ã£o/Bible/
â”‚   â””â”€â”€ sblgnt/                         # Arquivos MorphGNT (NT grego)
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                                # VariÃ¡veis de ambiente (criar)
```

## ğŸš€ InÃ­cio RÃ¡pido

### 1. Configurar Ambiente

```bash
# Criar ambiente virtual
python -m venv .venv

# Ativar ambiente
# Windows:
.venv\Scripts\activate
# Linux/Mac:
source .venv/bin/activate

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Configurar VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```bash
# === Provedor de LLM ===
# OpÃ§Ãµes: OPENAI, ANTHROPIC, COHERE, HF
LLM_PROVIDER=OPENAI

# === Chaves de API (configure apenas o provider escolhido) ===
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
COHERE_API_KEY=...
HF_API_TOKEN=hf_...

# === Hugging Face (se LLM_PROVIDER=HF) ===
HF_MODEL=gpt2  # ou outro modelo disponÃ­vel
```

### 3. Processar Corpus e Construir Ãndices

**IMPORTANTE**: Execute este comando antes da primeira utilizaÃ§Ã£o:

```bash
python scripts/setup_corpus.py
```

Este script irÃ¡:
- Processar textos SBLGNT (NT grego) para JSON
- Gerar embeddings com Sentence Transformers
- Construir Ã­ndice FAISS para busca rÃ¡pida
- Testar a busca semÃ¢ntica

â±ï¸ **Tempo estimado**: 5-10 minutos (depende do hardware)

### 4. Iniciar AplicaÃ§Ã£o

```bash
python src/app.py
```

Acesse: `http://localhost:8000`

## ğŸ“¡ Endpoints da API

### POST `/ask`
Pergunta sobre estudos bÃ­blicos usando o LLM configurado.

```json
{
  "question": "Qual o significado de á¼€Î³Î¬Ï€Î· no Novo Testamento?"
}
```

### POST `/find-similar`
Busca versos similares semanticamente.

```json
{
  "query": "á¼€Î³Î¬Ï€Î· Î¸ÎµÎ¿á¿¦",
  "top_k": 5
}
```

**Resposta:**
```json
{
  "query": "á¼€Î³Î¬Ï€Î· Î¸ÎµÎ¿á¿¦",
  "results": [
    {
      "book": "John",
      "chapter": 3,
      "verse": 16,
      "text": "ÎŸá½•Ï„Ï‰Ï‚ Î³á½°Ï á¼ Î³Î¬Ï€Î·ÏƒÎµÎ½ á½ Î¸Îµá½¸Ï‚ Ï„á½¸Î½ ÎºÏŒÏƒÎ¼Î¿Î½...",
      "similarity_score": 0.87
    }
  ]
}
```

### POST `/explain-links`
Encontra links intertextuais e explica conexÃµes usando LLM.

```json
{
  "query": "á¼˜Î½ á¼€ÏÏ‡á¿‡ á¼¦Î½ á½ Î»ÏŒÎ³Î¿Ï‚",
  "top_k": 5
}
```

## ğŸ”§ ConfiguraÃ§Ã£o de Provedores

### OpenAI (PadrÃ£o)
```bash
LLM_PROVIDER=OPENAI
OPENAI_API_KEY=sk-...
```
Modelos: `gpt-4o-mini`, `gpt-4o`, `gpt-4-turbo`

### Anthropic (Claude)
```bash
LLM_PROVIDER=ANTHROPIC
ANTHROPIC_API_KEY=sk-ant-...
```
Modelos: `claude-2.1`, `claude-3-opus`, `claude-3-sonnet`

### Cohere
```bash
LLM_PROVIDER=COHERE
COHERE_API_KEY=...
```
Modelos: `command`, `command-xlarge-nightly`

### Hugging Face
```bash
LLM_PROVIDER=HF
HF_API_TOKEN=hf_...
HF_MODEL=meta-llama/Llama-2-7b-chat-hf
```

## ğŸ§ª Testando

```bash
# Testar processamento de corpus
python src/services/corpus_processor.py

# Testar motor de intertextualidade
python src/services/intertextuality_engine.py

# Testar API (com servidor rodando)
curl -X POST http://localhost:8000/find-similar \
  -H "Content-Type: application/json" \
  -d '{"query": "á¼€Î³Î¬Ï€Î·", "top_k": 3}'
```

### Testes automatizados (pytest)

```bash
pytest -q
```

Se estiver sem o Ã­ndice FAISS, alguns testes retornam listas vazias por design.

## ğŸ“š DependÃªncias Principais

- **FastAPI**: Framework web assÃ­ncrono
- **Sentence Transformers**: GeraÃ§Ã£o de embeddings multilÃ­ngues
- **FAISS**: Busca vetorial eficiente (Facebook AI)
- **OpenAI/Anthropic/Cohere SDKs**: IntegraÃ§Ã£o com LLMs
- **PyTorch**: Backend para modelos de ML

## ğŸ—ºï¸ Roadmap

- [x] Suporte a mÃºltiplos LLMs
- [x] Busca semÃ¢ntica com FAISS
- [x] Processamento de SBLGNT (NT grego)
- [ ] Processamento de BHS (AT hebraico)
- [ ] IntegraÃ§Ã£o com textPAIR para validaÃ§Ã£o de paralelos
- [ ] IntegraÃ§Ã£o com SimAlign para alinhamento palavra-a-palavra
- [ ] Interface web interativa melhorada
- [ ] ExportaÃ§Ã£o de grafos de intertextualidade
- [ ] Cache de embeddings
- [ ] Suporte a Docker

## ğŸ–¥ï¸ Suporte a GPU (CUDA) e Ollama

### GPU (PyTorch + FAISS)
Para usar aceleraÃ§Ã£o NVIDIA:
1. Instale drivers NVIDIA e verifique com `nvidia-smi`.
2. Instale PyTorch com CUDA (exemplo CUDA 12.4):
  ```bash
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
  ```
3. Substitua `faiss-cpu` por `faiss-gpu` (preferencial via Conda/WSL):
  ```bash
  conda install -c pytorch faiss-gpu
  ```
4. Defina no `.env` (opcional) para migrar o Ã­ndice FAISS para GPU:
  ```bash
  USE_FAISS_GPU=1
  ```
5. Rode novamente o pipeline de corpus se quiser regenerar embeddings.

O cÃ³digo detecta automaticamente `cuda` via `torch.cuda.is_available()` e move o modelo de embeddings para GPU. Se `USE_FAISS_GPU=1` e FAISS GPU estiver disponÃ­vel, o Ã­ndice tambÃ©m Ã© migrado.

### Ollama (Modelos Locais)
Permite usar modelos locais (ex.: `llama3`, `mistral`, etc.) sem custo por token.

1. Instale Ollama:
  - Windows / WSL:
    ```bash
    curl -fsSL https://ollama.com/install.sh | bash
    ```
  - Ou instalador grÃ¡fico (se disponÃ­vel).
2. Baixe um modelo:
  ```bash
  ollama pull llama3
  ```
3. Ajuste `.env`:
  ```bash
  LLM_PROVIDER=OLLAMA
  OLLAMA_HOST=http://localhost:11434
  OLLAMA_MODEL=llama3
  ```
4. Inicie a API normal (`python src/app.py`).

O provider OLLAMA utiliza o endpoint local `POST /api/generate` para gerar respostas. Para trocar de modelo basta alterar `OLLAMA_MODEL` e reiniciar.

### Dicas de Performance
- Prefira modelos menores para prototipagem (7B / 8B).
- Regere embeddings apenas quando alterar o modelo de Sentence Transformers.
- Mantenha `normalize_embeddings=True` para melhor consistÃªncia na similaridade coseno.
- Use GPU apenas se o tamanho do corpus justificar (reduz latÃªncia em consultas grandes).

## ğŸ“„ LicenÃ§a

MIT

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, abra uma issue ou PR.

## ğŸ§± Arquitetura de Providers LLM

Os provedores LLM foram desacoplados via interface em `src/providers/`:
- `llm_base.py` (interface `LLMProvider` + `DummyProvider`)
- `openai_provider.py`, `anthropic_provider.py`, `cohere_provider.py`, `hf_provider.py`, `ollama_provider.py`

O serviÃ§o principal (`src/services/bible_service.py`) injeta o provider conforme `LLM_PROVIDER`.

## ğŸ³ Docker

Um `Dockerfile` foi adicionado para facilitar a execuÃ§Ã£o em container:

```bash
# Build
docker build -t an-agent-biblestudy .

# Run (porta 8000)
docker run -p 8000:8000 an-agent-biblestudy
```

Notas:
- O container nÃ£o gera o Ã­ndice por padrÃ£o. Para construir dentro do container, execute `python scripts/setup_corpus.py` interativamente.
- Monte volumes para `data/` e `indexes/` caso queira persistÃªncia fora do container.
